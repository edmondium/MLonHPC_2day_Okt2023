{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "324b17e8",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Note to teachers (click me)</summary>\n",
    "\n",
    "The data required for this notebook is stored in `/archive/casparl/packed_data_formats_data.tar.gz`. Unpack it in the `JHS_data` folder for the course when you want to use this notebook.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f143df9d",
   "metadata": {},
   "source": [
    "# Packed Data Formats\n",
    "\n",
    "In this tutorial we will review two ways of saving our data that is then loaded in PyTorch\n",
    "\n",
    "\n",
    "## Scenario: working with ImageNet data\n",
    "Imagine we want to train a network which can classify images accurately. Here we will use the ImageNet dataset to train our network. ImageNet is a huge image database of 14 million images. Below are a few example images from this dataset:\n",
    "\n",
    "<img src=\"src/imagenet.jpg\" alt= \"ImageNet\" width=\"500\" height=\"500\">\n",
    "\n",
    "So you download the dataset to your computer and unzip to see the images inside. However, extracting 14 million images is something your computer does not like for a couple of reasons:\n",
    "- extracting individual files takes long\n",
    "- all files are stored individually with their own metadata\n",
    "- on Snellius, we mostly have a shared disk so whatever data-intense heavy tasks are running may impact other users\n",
    "\n",
    "In this tutorial, you will learn how to work with large amounts of samples in a way that ensures reasonable performance for you _and_ other users on the cluster. We have selected 10000 random samples from the ImageNet dataset, and resampled them to the same resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e814f1",
   "metadata": {},
   "source": [
    "## Python imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9ad05d",
   "metadata": {},
   "source": [
    "Here, we import the python packages which we will be using throughout the notebook. We also se the path to the dataset that we will work on. You don't need to change anything here, just execute this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed5ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "#DATA_PATH = os.getenv(\"TEACHER_DIR\", os.getcwd()) + \"/JHS_data\"\n",
    "DATA_PATH = \"/projects/0/hpmlprjs/course_materials/packed_data_format/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddb0960",
   "metadata": {},
   "source": [
    "## Data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2693cc71",
   "metadata": {},
   "source": [
    "Let's check the dimensions of these images, and visualize some of the images from this dataset. You may notice: we have done some preprocessing on these images by rescaling them to a uniform resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c981726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a convenience function to print the image dimension and display it\n",
    "def print_dim_and_display(image_name):\n",
    "    image_path = os.path.join(DATA_PATH, 'disk_transformed', image_name)\n",
    "    img = PIL.Image.open(image_path)\n",
    "    print(f\"Image {image_name} has dimensions {img.size}\")\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dim_and_display('img00000000.JPEG')\n",
    "print_dim_and_display('img00000001.JPEG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7062c5e8",
   "metadata": {},
   "source": [
    "## Create a torch Dataset class for jpeg images\n",
    "\n",
    "To load the JPEG images during training, we write our own Dataset class. A PyTorch dataset class _has_ to implement three methods: `__init__`, `__len__` and `__get_item__`. We won't go into detail here on _how_ to write a Custom Dataset class for your dataset (the official PyTorch documentation has a good description of that https://pytorch.org/tutorials/beginner/basics/data_tutorial.html ), but simply provide you with one.\n",
    "\n",
    "Read it carefully and see if you can understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JpegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for Jpeg images to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Store all names of the images in the zip in self.img_names\n",
    "        self.img_names = self._get_img_names()\n",
    "    \n",
    "        # Read the labels from the zip file into self.labels\n",
    "        label_fname = \"validation_labels_10k.txt\"\n",
    "        self.labels = self._read_label_file(label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        return sorted(glob.glob(f\"{self.path}/*.JPEG\"))\n",
    "        \n",
    "    def _read_label_file(self, label_fname):\n",
    "        with open(os.path.join(self.path, label_fname), \"r\") as file:\n",
    "            return [int(l) for l in file.readlines()]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(img_name):\n",
    "        image = PIL.Image.open(img_name).convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name= self.img_names[index]\n",
    "        image = self.read_image(img_name)\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df97cc",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d96bd5",
   "metadata": {},
   "source": [
    "- The labels are read from the file `validation_labels_10k.txt`. How often is this file read: once for the entire training, or once per epoch?\n",
    "\n",
    "...\n",
    "\n",
    "- The images are read by the `read_image` function, called from the `__getitem__` function. How often is each image read from disk: once for the entire training, or once per epoch?\n",
    "\n",
    "...\n",
    "\n",
    "- What happens if the `__getitem__` function contains heavy routines, e.g. if the transform function is very heavy?\n",
    "\n",
    "...\n",
    "\n",
    "- For a GPU based training: where is the `__getitem__` function executed, on GPU or CPU?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1c2aa",
   "metadata": {},
   "source": [
    "## Benchmark the Jpeg Dataset\n",
    "In a real-life deep learning scenario, we have a dataloader which on every iteration returns images and labels of size `batch_size`. By running this for 1 epoch, we ensure that every images is seen at least once by the network. in this benchmarking example, only the timings of getting the images and pushing it to device is relevant. For now, we don't define any neural network training, we _only_ do dataloading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ffec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(dataset, epochs, batch_size, num_workers, persistent_workers, pin_memory, device, shuffle=True, warm_start=False):\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "            dataset,\n",
    "            num_workers=num_workers,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            persistent_workers=num_workers > 0 and persistent_workers,\n",
    "            pin_memory=pin_memory,\n",
    "        )\n",
    "    \n",
    "    if not warm_start:\n",
    "            start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        if epoch == 1 and warm_start:\n",
    "            start = time.time()\n",
    "        timer_per_epoch = time.time()\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = map(\n",
    "                    lambda tensor: tensor.to(device, non_blocking=pin_memory),\n",
    "                    (images, labels),\n",
    "                )\n",
    "\n",
    "        print(f\"Epoch {epoch} finished in {time.time() - timer_per_epoch}\")\n",
    "    total_time = time.time() - start\n",
    "    return total_time "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b95dc3",
   "metadata": {},
   "source": [
    "## Initialize some dataloading parameters\n",
    "Read careful and feel free to experiment with this. If you want to know more about the dataloader arguments, you can look them up in the official documentation at https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7e1a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_kwargs = {\n",
    "    \"epochs\": 2,\n",
    "    \"num_workers\": 1,\n",
    "    \"batch_size\": 16,\n",
    "    \"device\": \"cpu\",\n",
    "    \"persistent_workers\": True,\n",
    "    \"warm_start\": False,\n",
    "    \"pin_memory\": True,\n",
    "    \"shuffle\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07798507",
   "metadata": {},
   "source": [
    "## Define a transform function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d4c950",
   "metadata": {},
   "source": [
    "We include a transformation to imitate real-life applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0eb0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(new_size, to_tensor=False):\n",
    "    transform_list = []\n",
    "    if to_tensor:\n",
    "        transform_list.append(transforms.ToTensor())\n",
    "    transform_list.append(transforms.Resize((new_size, new_size), antialias=True))\n",
    "    transform_list.append(transforms.RandomHorizontalFlip())\n",
    "    transform = transforms.Compose(transform_list)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bc5aa1",
   "metadata": {},
   "source": [
    "## Run the benchmark on the Jpeg data\n",
    "Below, we\n",
    "- define the transform\n",
    "- initialize the JpegDataset object (passing it the defined transform)\n",
    "- run the benchmark for two epochs, returning the timing for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cdb23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = f\"{DATA_PATH}/disk_transformed\"\n",
    "\n",
    "transform_jpeg = transform(new_size=256, to_tensor=True)\n",
    "jpeg_dataset = JpegDataset(img_path, transform=transform_jpeg)\n",
    "\n",
    "jpeg_time = benchmark(jpeg_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443af47e",
   "metadata": {},
   "source": [
    "Results here might vary quite a bit. A typical result would be that Epoch 0 takes 200 seconds, and Epoch 1 takes about 30 seconds.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f401718",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53bf172",
   "metadata": {},
   "source": [
    "- Why do you think results vary? (hint: where are these files stored? Are you the only one using that filesystem?)\n",
    "\n",
    "...\n",
    "\n",
    "- Can you think of a reason why the second time is much faster?\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2859e0",
   "metadata": {},
   "source": [
    "## Packed data formats\n",
    "As we have already discussed, storing and loading a lot of individual files can lead to poor performance for both the filesystem as well as your network training. Packed data formats are data formats that store multiple samples in the same file. The simplest, and possibly most well known example, is a zip or tar file. However, there are many packed dataformats, such as HDF5, lmdb, petastorm and TFRecords.\n",
    "\n",
    "Reading samples from a packed file format is much friendlier on the filesystem, because there is _only_ one file for which the filesystem needs to lookup the metadata. An additional advantage is that it is much easier to manage one (or a few) files, than millions of individual files. Simply listing a folder with 10k images with `ls` takes a few seconds - imagine if you have millions of samples!\n",
    "\n",
    "Instead of a Jpeg dataset class, let's make a dataclass for zip files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aa239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZIPDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for packed ZIP to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "        load_encoded (bool): whether the images within the .zip file are encoded or saved as bytes directly\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None, load_encoded=False):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.load_encoded = load_encoded\n",
    "\n",
    "        self.zip_file = zipfile.ZipFile(path)\n",
    "        self.members = sorted(self.zip_file.namelist())\n",
    "\n",
    "        # Store all names of the images in the zip in self.img_names\n",
    "        self.img_names = self._get_img_names()\n",
    "\n",
    "        # Read the labels from the zip file into self.labels\n",
    "        label_fname = \"dataset.json\"\n",
    "        self.labels = self._get_labels(label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        PIL.Image.init()\n",
    "        return [m for m in self.members if m.lower().endswith(tuple(PIL.Image.EXTENSION.keys()))]\n",
    "\n",
    "    def _get_labels(self, label_fname):\n",
    "        label_file = self.zip_file.open(label_fname, \"r\")\n",
    "        return json.load(label_file)[\"labels\"]\n",
    "\n",
    "\n",
    "    def _get_image(self, img_fname, shape=(3, 256, 256)):\n",
    "        img_f = self.zip_file.open(img_fname, \"r\")\n",
    "        if self.load_encoded:\n",
    "            image = PIL.Image.open(img_f).convert(\"RGB\")\n",
    "        else:\n",
    "            buffer = np.frombuffer(img_f.read(), dtype=np.uint8).reshape(shape)\n",
    "            image = torch.tensor(buffer) # make writeable copy\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_fname = self.img_names[index]\n",
    "        image = self._get_image(img_fname)\n",
    "        label = self.labels[img_fname]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09eeb08",
   "metadata": {},
   "source": [
    "## Benchmark the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f26cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = f\"{DATA_PATH}/zip/part0.zip\"\n",
    "transform_zip = transform(new_size=256, to_tensor=False)\n",
    "zip_dataset = ZIPDataset(zip_path, transform=transform_zip, load_encoded=False)\n",
    "zip_time = benchmark(zip_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = f\"{DATA_PATH}/zip_encoded/part0.zip\"\n",
    "transform_zip = transform(new_size=256, to_tensor=True)\n",
    "zip_dataset = ZIPDataset(zip_path, transform=transform_zip, load_encoded=True)\n",
    "zip_time = benchmark(zip_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffaadc6",
   "metadata": {},
   "source": [
    "Although results on the zip-file can still vary depending on the load other users put on the filesystem, it is likely more predictable. A typical result would be that Epoch 0 takes 60 seconds, and Epoch 1 takes about 50 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5341d",
   "metadata": {},
   "source": [
    "## Comparing the results to the individual JPEG approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9a70fc",
   "metadata": {},
   "source": [
    "As you can see, the JPEG approach is slower, _but only for the first epoch_. So: you might be a bit confused now. Didn't we say that packed file formats are a good idea?\n",
    "\n",
    "Well, that's because _they are_. The only reason the dataset with individual JPEGs was faster in this case is because our filesystem managed to cache them. The problem here is twofold:\n",
    "- First epoch, you are still _heavily_ hitting the filesystem, which may slow it down substantially for other users\n",
    "- Whether the dataset will be cached is unpredictable, as it depends on several factors (size of the dataset, but also how much other users are using the cache)\n",
    "\n",
    "With both points, the issue is that if _one_ user does it, it is usually not a problem. But an HPC system can have hundreds of users, and if they _all_ do this, performance will be greatly affected. \n",
    "\n",
    "You might wonder: why did I never worry about this when training networks on my own laptop? The reason is twofold: \n",
    "1) the disk in your laptop is used _only_ by you. Thus, you are not competing for I/O performance.\n",
    "2) the disk in your laptop is _local_. Serving files over a network, especially if these are small files, result in higher latency and thus fewer file reads per second.\n",
    "\n",
    "One final note: it is good to realize that as long as your compute takes longer than preloading the next batch from disk _you are never waiting for IO_. Suppose that in the above example, the compute time for one epoch would be 90 seconds. That would mean that with the ZIP-dataloader, you would _never_ wait for I/O, whereas with the JPEG-dataloader, you would be waiting for I/O whenever the dataset can _not_ be read from cache."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a90bb1",
   "metadata": {},
   "source": [
    "# But can we do better? Multiworker support!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13a689",
   "metadata": {},
   "source": [
    "Up until this point, we've only used a single thread to do the dataloading. However, the dataloaders are _also_ in charge of the transformations. I.e. the transformations as defined by the `transform` function are executed by the dataloading threads. If these take a lot of time, it can be useful to have _more_ threads working on this. Moreover: by reading multiple samples simultaneously, you are often able to get a higher aggregate I/O from the filesystem.\n",
    "\n",
    "Let's change the dataloader settings to increase the number of processes to say 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the number of workers to 8\n",
    "dataloader_kwargs[\"num_workers\"] = 8\n",
    "# Re-run the JPEG dataset benchmark and \n",
    "jpeg_time_multi = benchmark(jpeg_dataset, **dataloader_kwargs)\n",
    "# Re-run the ZIP dataset benchmark and call the benchmark time\n",
    "zip_time_multi = benchmark(zip_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eec4203",
   "metadata": {},
   "source": [
    "Ouch... We got a weird BadZipFile error from ZIP with two file names having different headers or a bad magic number for file header... What could this be?\n",
    "\n",
    "\n",
    "What is happening is that the images are all zipped in a single ZIP file. While loading the data, we set the number of processes that read the data to 8 (where did we set it?). All these processes are trying to open and read using the same file handle. This causes the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9673ed2a",
   "metadata": {},
   "source": [
    "## ReWriting the ZIP Dataset class for multi-worker support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffade99",
   "metadata": {},
   "source": [
    "In our previous `__init__`, the `self.zip_file = zipfile.ZipFile(path)` line created a single file handle, which was used by all workers. This does not work if we have multiple processes: one thing a file handle keeps track of is where in the file it is reading. If multiple processes are reading at the same time, this causes conflict.\n",
    "\n",
    "Thus, we need to adapt the ZIP dataset class so that each worker gets its own file handle to the same zipfile. That way, each worker can read from its own position in the file, without affecting others.\n",
    "\n",
    "This is done in the following four steps:\n",
    " - Create the workers\n",
    " - Get the information about the workers (the worker-id)\n",
    " - Give each worker a separate handle\n",
    " - Read the images and labels using this handle\n",
    " \n",
    " Tips:\n",
    " - Multiple workers are only created _after_ the `__init__`, so the creation of the file handle _per worker_ has to be triggered by the `__getitem__`.\n",
    " - Fill in the new `_get_file_handle` function\n",
    " - Revise the code from the previous dataset for loading the labels and images\n",
    " \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac875d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiWorkerZIPDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for packed ZIP to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "        load_encoded (bool): whether the images within the .zip file are encoded or saved as bytes directly\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None, load_encoded=False):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.load_encoded = load_encoded\n",
    "\n",
    "        # We get a temporary file handle, just to be able to read _what_ files are in the zip\n",
    "        zip_handle = zipfile.ZipFile(path)\n",
    "        self.members = sorted(zip_handle.namelist())\n",
    "        \n",
    "        # We also create a dict of zip handles, which will hold a zip handle for each worker later on\n",
    "        self.zip_handle = {}\n",
    "\n",
    "        # Store all names of the images in the zip in self.img_names\n",
    "        self.img_names = self._get_img_names()\n",
    "\n",
    "        # Read the labels from the zip file into self.labels\n",
    "        label_fname = \"dataset.json\"\n",
    "        self.labels = self._get_labels(zip_handle, label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        PIL.Image.init()\n",
    "        return [m for m in self.members if m.lower().endswith(tuple(PIL.Image.EXTENSION.keys()))]\n",
    "\n",
    "    def _get_labels(self, zip_handle, label_fname):\n",
    "        label_file = zip_handle.open(label_fname, \"r\")\n",
    "        return json.load(label_file)[\"labels\"]\n",
    "\n",
    "    def _get_file_handle(self, fname):\n",
    "        # TODO: \n",
    "        # This function must return the file handle for the current worker, \n",
    "        # or create one if it is not already created\n",
    "        worker = torch.utils.data.get_worker_info()\n",
    "        worker = worker.id if worker else None\n",
    "\n",
    "        if worker not in self.zip_handle:\n",
    "            self.zip_handle[worker] = zipfile.ZipFile(self.path)\n",
    "\n",
    "        return self.zip_handle[worker].open(fname, \"r\")\n",
    "\n",
    "    def _get_image(self, img_fname, shape=(3, 256, 256)):\n",
    "        img_f = self._get_file_handle(img_fname)\n",
    "        if self.load_encoded:\n",
    "            image = PIL.Image.open(img_f).convert(\"RGB\")\n",
    "        else:\n",
    "            buffer = np.frombuffer(img_f.read(), dtype=np.uint8).reshape(shape)\n",
    "            image = torch.tensor(buffer) # make writeable copy\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_fname = self.img_names[index]\n",
    "        image = self._get_image(img_fname)\n",
    "        label = self.labels[img_fname]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean all file handles of the workers on exit\"\"\"\n",
    "        if hasattr(self, \"zip_handle\"):\n",
    "            for o in self.zip_handle.values():\n",
    "                o.close()\n",
    "\n",
    "    def __getstate__(self):\n",
    "        \"\"\"Serialize without the ZipFile references, for multiprocessing compatibility\"\"\"\n",
    "        state = dict(self.__dict__)\n",
    "        state[\"zip_handle\"] = {}\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c7deb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_dataset_multi = MultiWorkerZIPDataset(zip_path, transform=transform_zip, load_encoded=True)\n",
    "zip_time_multi = benchmark(zip_dataset_multi, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d5742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "speed_up = zip_time / zip_time_multi\n",
    "print(f'Speed up of zip dataset using {dataloader_kwargs[\"num_workers\"]} workers: {round(speed_up, 3)}x faster')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05718473",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "We have prepared a tiny dataset of just 10 JPEG images in the `Day2/notebooks/data` directory. The labels are stored in a `.json` file, which has the same structure as the one in the ZipDataset we've used above.\n",
    "\n",
    "The exercise is to run the benchmark first on the individual JPEG images, then on the zip file. Note that the timings for such a tiny dataset are not reliable at all, and we'll ignore them here: this exercise is just to practice and see what you need to do to convert things from a JPEG Dataset to a Zip Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e53c506",
   "metadata": {},
   "source": [
    "First, you'll need to adapt the label reading for the jpeg dataset. Our initial JpegDataset assumes the labels are in a `txt` file, and that the first entry just corresponds to the first JPEG image. That is not the case here: here, we have the labels in a `json` file, and the labels are stored as key-value pairs, with the filenames as key.\n",
    "\n",
    "Let's see what is in the `.json` file, so that you know how to extract the labels from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16430f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMEDIR = os.getenv('HOME')\n",
    "#json_file = os.path.join(HOMEDIR, 'JHS_notebooks', 'Day2', 'notebooks', 'data', 'dataset.json')\n",
    "json_file = os.path.join(HOMEDIR, 'Courses', 'MLonHPC_2day_Okt2023', 'Day2', 'notebooks', 'data', 'dataset.json')\n",
    "with open(json_file, \"r\") as file:\n",
    "    print(json.load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530f9e0b",
   "metadata": {},
   "source": [
    "## Exercise 1.a\n",
    "\n",
    "We start of with the regular JpegDataset, and have indicated where you will need to make the changes. You can get inspiration from how we read the labels from the `json` file in the ZipDataset above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2b603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JpegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for Jpeg images to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Store all names of the images in the zip in self.img_names\n",
    "        self.img_names = self._get_img_names()\n",
    "\n",
    "        # Read the labels from the zip file into self.labels\n",
    "## ====================================================\n",
    "## CHANGE IS NEEDED HERE:\n",
    "        label_fname = \"validation_labels_10k.txt\"\n",
    "## ====================================================\n",
    "        self.labels = self._read_label_file(label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        return sorted(glob.glob(f\"{self.path}/*.JPEG\"))\n",
    "        \n",
    "    def _read_label_file(self, label_fname):\n",
    "        with open(os.path.join(self.path, label_fname), \"r\") as file:\n",
    "## ====================================================\n",
    "## CHANGES IS NEEDED HERE:\n",
    "            return [int(l) for l in file.readlines()]\n",
    "## ====================================================\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(img_name):\n",
    "        image = PIL.Image.open(img_name).convert(\"RGB\")\n",
    "        return image\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name= self.img_names[index]\n",
    "        image = self.read_image(img_name)\n",
    "## ====================================================\n",
    "## CHANGES IS NEEDED HERE:\n",
    "## Hint: img_name is the full path to the image, while the keys in the json file are the filenames\n",
    "## You can get the filename from the full path using os.path.basename()\n",
    "        label = self.labels[index]\n",
    "## ====================================================\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6dc60e",
   "metadata": {},
   "source": [
    "## Solution 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd61aaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JpegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"ImageNet10k Dataset for Jpeg images to pass to a PyTorch dataloader\n",
    "\n",
    "    Args:\n",
    "        path (str): location where the images are stored on disk\n",
    "        transform (obj): torchvision.transforms object or None\n",
    "    Returns:\n",
    "        torch Dataset: to pass to a dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "        # Store all names of the images in the zip in self.img_names\n",
    "        self.img_names = self._get_img_names()\n",
    "    \n",
    "        # Read the labels from the zip file into self.labels\n",
    "        label_fname = \"dataset.json\"\n",
    "        self.labels = self._get_labels(label_fname)\n",
    "\n",
    "    def _get_img_names(self):\n",
    "        return sorted(glob.glob(f\"{self.path}/*.JPEG\"))\n",
    "        \n",
    "    def _get_labels(self, label_fname):\n",
    "        # label_file = zip_handle.open(label_fname, \"r\")\n",
    "        with open(os.path.join(self.path, label_fname), \"r\") as file:\n",
    "            return json.load(file)[\"labels\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def read_image(img_name):\n",
    "        #image = PIL.Image.open(img_name).convert(\"RGB\")\n",
    "        image = PIL.Image.open(img_name)\n",
    "        return image\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_name= self.img_names[index]\n",
    "        image = self.read_image(img_name)\n",
    "        label = self.labels[os.path.basename(img_name)]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f014c413",
   "metadata": {},
   "source": [
    "## Exercise 1.b\n",
    "\n",
    "Now, create the JpegDataset instance and run the benchmark function - similar tohow we did it above. We've already defined the correct path for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e644335",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMEDIR = os.getenv('HOME')\n",
    "#img_path = os.path.join(HOMEDIR, 'JHS_notebooks', 'Day2', 'notebooks', 'data')\n",
    "img_path = os.path.join(HOMEDIR, 'Courses', 'MLonHPC_2day_Okt2023', 'Day2', 'notebooks', 'data')\n",
    "\n",
    "jpeg_dataset = ...\n",
    "\n",
    "jpeg_time = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25373dc",
   "metadata": {},
   "source": [
    "## Solution 1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8e1181",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMEDIR = os.getenv('HOME')\n",
    "#img_path = os.path.join(HOMEDIR, 'JHS_notebooks', 'Day2', 'notebooks', 'data')\n",
    "img_path = os.path.join(HOMEDIR, 'Courses', 'MLonHPC_2day_Okt2023', 'Day2', 'notebooks', 'data')\n",
    "\n",
    "jpeg_dataset = JpegDataset(img_path, transform=transform_jpeg)\n",
    "\n",
    "jpeg_time = benchmark(jpeg_dataset, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4a32db",
   "metadata": {},
   "source": [
    "## Exercise 1.c\n",
    "\n",
    "Next, you'll have to zip the imaging data. The easiest way to do this is on the command line. \n",
    "- Open a Terminal from the main Notebook Server (where you see all the files in your home directory)\n",
    "- Use `cd` to change directory to the `JHS_notebooks/Day2/notebooks/data` subdirectory in your homedir\n",
    "- Run `zip -0 myzip.zip img0000000* dataset.json`\n",
    "\n",
    "N.B. with `-0` you store files in a zip without compressing them. This uses more disk space, but less CPU when extracting the data. Since images are hardly compressible anyway, it makes sense to store them without compression. For highly compressible data (like text) it _might_ be worth compressing in order to save some disk space - as long as you have enough (CPU) workers to do the decompression work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70f0701",
   "metadata": {},
   "source": [
    "## Exercise 1.d\n",
    "\n",
    "Create a `MultiWorkerZIPDataset`, and run the `benchmark()` function on it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d8002",
   "metadata": {},
   "outputs": [],
   "source": [
    "HOMEDIR = os.getenv('HOME')\n",
    "#zip_path = os.path.join(HOMEDIR, 'JHS_notebooks', 'Day2', 'notebooks', 'data', 'myzip.zip')\n",
    "zip_path = os.path.join(HOMEDIR, 'Courses', 'MLonHPC_2day_Okt2023', 'Day2', 'notebooks', 'data', 'myzip.zip')\n",
    "\n",
    "zip_dataset_multi = MultiWorkerZIPDataset(zip_path, transform=transform_zip, load_encoded=True)\n",
    "zip_time_multi = benchmark(zip_dataset_multi, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c5906a",
   "metadata": {},
   "source": [
    "# TODO: insert an exercise in which we run e.g. on a torchvision dataset. These do not use packed formats, so what can we do? (apart from going into the code and heavily modifying it...) => Solution: use local scratch disks (only an option if dataset is small enough to fit scratch disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360df97",
   "metadata": {},
   "source": [
    "# Recap\n",
    "\n",
    "- Dataloading individual JPEGs is slower, unless they are cached\n",
    "- Dataloading from a packed file format provides more predictable performance\n",
    "- Dataloading from a packed file format puts less load on the filesystem. This is important in HPC clusters, where a lot of users are using the same filesystem.\n",
    "- When we cannot control the dataset format (e.g. when using torchvision datasets), we can store the dataset on scratch disks - assuming the dataset is small enough. Since scratch disks are local to the node, they are shared between way fewer users, and don't incur the overhead of accessing the file over a network.\n",
    "\n",
    "\n",
    "### Final remarks\n",
    "Try to change the number of workers to any number between 1 and 32, see what happens with the speed-up!\n",
    "\n",
    "Other tips:\n",
    "- Any transformation on your dataset which is deterministic (for example resize) can be done once in advance as preprocessing and will probably save you time during the actual training.\n",
    "- Look into LMDB/HDF5/Petastorm to see if there is a packed data format which may be more suited for your data\n",
    "- More info here: https://servicedesk.surf.nl/wiki/display/WIKI/Best+Practice+for+Data+Formats+in+Deep+Learning and here: \\newline https://github.com/sara-nl/Packed-Data-Formats\n",
    "\n",
    "\n",
    "Finally, it's good to keep in mind that reading files is a task which we can partially do in a overlapping way with GPU computations. In this hands-out, we did not consider a neural network so the GPU was idle at all times. In real-life scenarios, we can push the data to the GPU, let the GPU do its calculations and at the same time load some more data for the next batch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32431f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
